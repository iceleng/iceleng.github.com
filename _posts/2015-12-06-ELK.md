---
layout: post
title:  ELK学习
categories: [elk]
---

2015-12-23
===
关于kibana中正则表达式的应用
+ 对于需要查询某field是以指定字符串开始或者结尾，需要使用正则表达式
+ 所查询的field必须是raw类型，即未经分词的
+ kibana中使用/作为正则表达式的开始和结束符
+ 对于搜索指定url开头或者结尾的，注意使用转义符
+ 比如：搜索/otc/cir开头的url，需要这样写 request:/\/otc\/cir\/.*/

2015-12-18
===
The query language allows also some more fine-grained search queries, like:
---
lang:en	to just search inside a field named “lang”
lang:e?	wildcard expressions
lang:(en OR es)	OR queries on fields
user.listed_count:[0 TO 10]	range search on numeric fields
lang:/e[ns]/	regular expression search (very slow, only do this if really necessary!)
The full documentation of the query language can be found in the [elasticsearch documentation] (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#query-string-syntax).

Loading the Sample Datasetedit
---
You can download the sample dataset (accounts.json) from here. Extract it to our current directory and let’s load it into our cluster as follows:

	curl -XPOST 'localhost:9200/bank/account/_bulk?pretty' --data-binary "@accounts.json"
	curl 'localhost:9200/_cat/indices?v'
	curl 'localhost:9200/_cat/indices?v'
	health index pri rep docs.count docs.deleted store.size pri.store.size
	yellow bank    5   1       1000            0    424.4kb        424.4kb

[Kibana 4 Tutorial – Part 3: Visualize] (https://www.timroes.de/2015/02/07/kibana-4-tutorial-part-3-visualize/)
---


2015-12-16
===
关于从snapshot中恢复数据的尝试，根据官方文档步骤如下：
---
1. 在elasticsearch中配置repo变量（原始的配置文档是没有这部分内容的

		cat ../config/elasticsearoh.yml | grep rep 
		#ice add shared file system reposityory 2015-12-15
		path.repo: ["/home/iceleng/developerTools/elasticsearch-2.1.0/repo"]
2. 在elasticsearch中注册snapshot（必须有前面指定的path.repo）

		curl -XPUT 'http://localhost:9200/_snapshot/my_backup' -d '{"type":"fs","settings":{"location":"/home/iceleng/developerTools/elasticsearch-2.1.0/repo","compress":true}}'
		{"acknowledged":true}
3. 查看snapshot

		curl localhost:9200/_snapshot
		{"my_backup":{"type":"fs","settings":{"compress":"true","location":"/home/iceleng/developerTools/elasticsearch-2.1.0/repo"}}}
		curl localhost:9200/_snapshot?pretty
		{
		  "my_backup" : {
		    "type" : "fs",
		    "settings" : {
		      "compress" : "true",
		      "location" : "/home/iceleng/developerTools/elasticsearch-2.1.0/repo"
		    }
		  }
		}
4. 将kibana4教程中提到的twitter数据和bank数据的相关文件copy到repo目录
5. 查看拷贝到repo目录中的文件

		ls
		index  indices  metadata-v1  snapshot-v1
6. 根据命名规则，可以知道这个snapshot的名字是v1
7. 将snapshot的数据restore到index

	 	curl -XPOST http://localhost:9200/_snapshot/my_backup/v1/_restore
8. 将当前index创建snapshot

		curl -XPOST http://localhost:9200/_snapshot/my_backup/ice_snapshot?wait_for_competion=true

#ELK相关资料文档
[Elastic中文社区](http://elasticsearch.cn/)  
[kibana4 教程](https://www.timroes.de/2015/02/07/kibana-4-tutorial-part-3-visualize/)  
[logstash自定义插件和资料](http://blog.csdn.net/earbao/article/details/49335217)  
[Logstash 最佳实践](http://udn.yyuap.com/doc/logstash-best-practice-cn/index.html)  
[运维生存时间 elk文章](http://www.ttlsa.com/log-system/elk/) 

##nginx带upstream信息的log配置
	log_format  combine '$remote_addr - $remote_user [$time_local] "$request" $http_host ' 
	'$status $body_bytes_sent "$http_referer" '
	'"$http_user_agent" "$http_x_forwarded_for" '
	'$upstream_addr $upstream_status $upstream_cache_status "$upstream_http_content_type" $upstream_response_time > $request_time';


##logstash nginx配置文件
	input{
	  stdin{}
	  file{
			type=>"nginx-access"
	    path=>"/var/log/nginx/iceleng.access.log"
	    start_position=>"beginning"
	  }
	}
	filter{
		grok{
			#patterns_dir=>"../patterns"
			#match=>{"message"=>"%{NGINXACCESS}"}
	    match =>{"message"=>"%{IPORHOST:client_ip} - - \[%{HTTPDATE:timestamp}\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:http_version})?|-)\" %{IPORHOST:domain} %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} (%{QS:x_forword}|-) (%{URIHOST:upstream_host}|-) (%{NUMBER:upstream_response}|-) (%{WORD:upstream_cache_status}|-) %{QS:upstream_content_type} (%{BASE16FLOAT:upstream_response_time}|-) > (%{BASE16FLOAT:request_time}|-)"}
	  }
	  date {
	    match => ["timestamp" , "dd/MMM/YYYY:HH:mm:ss Z"]
	  }
	}
	output{
	  stdout{}
	  elasticsearch{
	    hosts=>"192.168.1.7"
	  }
	}

##安装license
	./plugin install license-> Installing license...
	Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/license/2.1.0/license-2.1.0.zip ...
	Downloading .......DONE
	Verifying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/license/2.1.0/license-2.1.0.zip checksums if available ...
	Downloading .DONE
	Installed license into /home/iceleng/dev/elasticsearch-2.1.0/plugins/license
	
##安装es的marvel插件
	iceleng@iceleng-desktop ~/dev/elasticsearch-2.1.0/bin $ ./plugin install marvel-agent
	-> Installing marvel-agent...
	Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.1.0/marvel-agent-2.1.0.zip ...
	Downloading ..........DONE
	Verifying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.1.0/marvel-agent-2.1.0.zip checksums if available ...
	Downloading .DONE
	Installed marvel-agent into /home/iceleng/dev/elasticsearch-2.1.0/plugins/marvel-agent
	
##安装kibana的插件
	iceleng@iceleng-desktop ~/dev/kibana-4.3.0-linux-x86/bin $ ./kibana plugin --install elasticsearch/marvel/latest
	Installing marvel
	Attempting to extract from https://download.elastic.co/elasticsearch/marvel/marvel-latest.tar.gz
	Downloading 1497374 bytes....................
	Extraction complete
	Optimizing and caching browser bundles...
	Plugin installation complete
	
	iceleng@iceleng-desktop ~/dev/kibana-4.3.0-linux-x86 $ ls 
	bin     installedPlugins  node          optimize      README.txt  webpackShims
	config  LICENSE.txt       node_modules  package.json  src


##Elastic License邮件

    发件人：support@elastic.co保存到地址簿
    收件人：iceprogrammer@sohu.com
    时　间：2015年12月13日 下午8:40:11

	Thank you for registering for a free license! This Basic license expires on December 13, 2016.
	
	To download your license, please go to:
	
	--> http://license.elastic.co/registration/download/65c0f940-5528-41ac-acc5-0cfadb21327f
	
	For license installation instructions:
	
	--> https://www.elastic.co/guide/en/marvel/current/license-management.html
	
	If you have any questions or issues, please visit us on the forums: https://discuss.elastic.co/ or reach out to us directly at info@elastic.co.
	
	Best,
	The Elastic Team


##logstash定义elasticSearch的模板
保存进 Elasticsearch
<http://udn.yyuap.com/doc/logstash-best-practice-cn/get_start/full_config.html>

##logstash 向elasticsearch写入数据，如何指定多个数据template
<http://blog.csdn.net/zhifeiyu2008/article/details/47720847>
之前在配置从logstash写数据到elasticsearch时，指定单个数据模板没有问题，但是在配置多个数据模板时候，总是不成功，后来找了很多资料，终于找到解决办法，就是要多加一个配置项： template_name ，切该名字必须全部为小写。
参考配置信息：

    output {
            if [type] == "log_01" {
                    elasticsearch {
                            cluster => 'elasticsearch'
                            host =>         'x.x.x.x'
                            index => 'log_01-%{+YYYY-MM-dd}'
                            port => '9300'
                            workers => 1
                            template => "/data/logstash/conf/template_01.json"
                            template_name => "template_01.json"
                            template_overwrite => true
                    }
            }
            if [type] == "log_02" {
                elasticsearch {
                            cluster => 'elasticsearch'
                            host =>         'x.x.x.x'
                            index => 'log_02-%{+YYYY-MM-dd}'
                            port => '9300'
                            workers => 1
                            template => "/data/logstash/conf/template_02.json"
                            template_name => "template_01.json"
                          template_overwrite => true
                    }
            }
    }


##使用 ElasticSearch + LogStash + Kibana 来可视化网络流量，kibanaelasticsearch
http://www.bkjia.com/yjs/991319.html
为了让采集到的数据的类型能被 elasticsearch 正确处理，添加如下模板，自动对所有 logstash_netflow- 开头的索引采取指定的类型解析：

    curl -XPUT http://localhost:9200/_template/logstash_netflow -d '{
        "template" : "logstash_netflow-*",
        "settings": {
          "index.cache.field.type": "soft",
          "index.store.compress.stored": true
        },
        "mappings" : {
            "_default_" : {
               "_all" : {"enabled" : false},
               "properties" : {
                  "@message":     { "index": "analyzed", "type": "string"  },
                  "@source":      { "index": "not_analyzed", "type": "string"  },
                  "@source_host": { "index": "not_analyzed", "type": "string" },
                  "@source_path": { "index": "not_analyzed", "type": "string" },
                  "@tags":        { "index": "not_analyzed", "type": "string" },
                  "@timestamp":   { "index": "not_analyzed", "type": "date" },
                  "@type":        { "index": "not_analyzed", "type": "string" },
                  "netflow": {
                       "dynamic": true,
                       "path": "full",
                       "properties": {
                           "version": { "index": "analyzed", "type": "integer" },
                           "first_switched": { "index": "not_analyzed", "type": "date" },
                           "last_switched": { "index": "not_analyzed", "type": "date" },
                           "direction": { "index": "not_analyzed", "type": "integer" },
                           "flowset_id": { "index": "not_analyzed", "type": "integer" },
                           "flow_sampler_id": { "index": "not_analyzed", "type": "integer" },
                           "flow_seq_num": { "index": "not_analyzed", "type": "long" },
                           "src_tos": { "index": "not_analyzed", "type": "integer" },
                           "tcp_flags": { "index": "not_analyzed", "type": "integer" },
                           "protocol": { "index": "not_analyzed", "type": "integer" },
                           "ipv4_next_hop": { "index": "analyzed", "type": "ip" },
                           "in_bytes": { "index": "not_analyzed", "type": "long" },
                           "in_pkts": { "index": "not_analyzed", "type": "long" },
                           "out_bytes": { "index": "not_analyzed", "type": "long" },
                           "out_pkts": { "index": "not_analyzed", "type": "long" },
                           "input_snmp": { "index": "not_analyzed", "type": "long" },
                           "output_snmp": { "index": "not_analyzed", "type": "long" },
                           "ipv4_dst_addr": { "index": "analyzed", "type": "ip" },
                           "ipv4_src_addr": { "index": "analyzed", "type": "ip" },
                           "dst_mask": { "index": "analyzed", "type": "integer" },
                           "src_mask": { "index": "analyzed", "type": "integer" },
                           "dst_as": { "index": "analyzed", "type": "integer" },
                           "src_as": { "index": "analyzed", "type": "integer" },
                           "l4_dst_port": { "index": "not_analyzed", "type": "long" },
                           "l4_src_port": { "index": "not_analyzed", "type": "long" }
                       },
                       "type": "object"
                   }
                }
            }
       }
    }'






